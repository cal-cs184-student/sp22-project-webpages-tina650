<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<style>  
    div.padded {  
      padding-top: 0px;  
      padding-right: 100px;  
      padding-bottom: 0.25in;  
      padding-left: 100px;  
    }  
  </style> 
<title>Your Name  |  CS 184</title>
<meta http-equiv="content-type" content="text/html; charset=utf-8" />
<link rel="stylesheet" type="text/css" href="style.css" media="screen" />
</head>
<body>
<br />
<h1 align="middle">Assignment 3: PathTracer</h1>
    <h2 align="middle">Tina Li & Daniel Won, CS184-aax & CS184-ack</h2>
    <h3 align="middle"><a href="https://cal-cs184-student.github.io/sp22-project-webpages-tina650/proj3-1/index.html">https://cal-cs184-student.github.io/sp22-project-webpages-tina650/proj3-1/index.html</a></h3>

    <div class="padded">
        <p>
            In this project, we learned how to render realistic lighting via the concept of ray tracing and various illumination techniques including direct and global illumination as well as adaptive sampling that reduces noise. This project resonated with our prior knowledge and experience with graphics as ray tracing is an idea that has commonly popped up in the graphics, VFX, and gaming worlds that made hyper-realistic renders a reality, and it was very interesting to learn how ray tracing worked and the computations necessary to make ray-traced illumination function correctly.
        </p>
    <h2 align="middle">Part 1: Ray Generation and Intersection</h2>
        
        <p>
            In order to generate a ray, we first changed the hFov and vFov into radians. Then we calculated the max y and y’s by basically taking the tan of half the hFov and vFov. 
            Then, we offset the image by 0.5 x and y. We also then scaled it by dividing the max x and y by 0.5. Then we took those scaled values and multiplied them by the offsets. 
            We used these values to create the final ray. After that we set final ray’s min and max t values to nClip and fClip. Then we return final ray. 
        </p>
        
        <p align="middle"><pre align="middle">r(<i>t</i>) = o + <i>t</i>d</pre></p>
        <p align="middle"><pre align="middle">p: (p - c)^2 - R^2 = 0</pre></p>
    <p> Then we solve for intersection by setting</p>
    <p align="middle"><pre align="middle">(o + <i>t</i>d - c)^2 - R^2 = 0</pre></p>
    <p align="middle"><pre align="middle"></pre></p>
<p>
    In which we use r(<i>t</i>) = o + <i>t</i>d to find a, b, and c. We also use the quadratic formula to solve for t.
</p>
        
        <p>
            For our primitive intersections parts we implemented a Moller Trumbore Algorithm for ray-triangles 
            intersection and used the ray intersection with sphere. For the latter we used 
        </p>
        
        <div align="center">
            <table style="width=100%">
                <tr>
                    <td align="middle">
                    <img src="images/al.PNG" width="360px" />
                    <figcaption align="middle">Moller Trumbore Algorithm</figcaption>
                </tr>
            </table>
        </div>
        
        <p>
            We used the Moller Trumbore Algorithm to implement the ray-triangle intersection algorithms. We first made some Vector3D variables such as e1, e2, s, s1, and s2. 
            We then set a Vector3D for the resultant vector after we use the algorithm. Then from that vector we extracted t, b1, and b2. Then we checked if the point was in 
            the triangle using barycentric coordinates, by checking if b1 and b2 were positive and if b1 + b2 was smaller than 1. We also checked if the t was greater than 
            the ray’s min t and less than the ray’s max t. If the point was indeed inside the triangle, then we would set the ray’s max t to the resultant vector’s t and 
            return true. If not, then we returned false.
        </p>
      
        
        <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/plane.png" align="middle" width="360px" />
            <figcaption align="middle">Plane</figcaption>
          </td>
          <td>
            <img src="images/cube.png" align="middle" width="360px" />
            <figcaption align="middle">Cube</figcaption>
          </td>
        </tr>
        <br>
        <tr>
          <td>
            <img src="images/balls.png" align="middle" width="360px" />
            <figcaption align="middle">Balls</figcaption>
            </td>
            <td>
            <img src="images/gems.png" align="middle" width="360px" />
            <figcaption align="middle">Gems</figcaption>
          </td>
        </tr>
      </table>
    </div>

 <h2 align="middle">Part 2: Bounding Volume Hierarchy</h2>
     
     <p>
         For the most part, we followed the given specifications to implement our BVH construction 
         algorithm. The condition for which a given node is a leaf was if the number of primitives 
         that we iterated through was less than the provided max_leaf_size parameter, in which we 
         would simply set the start and end of the node to the given start and end iterators and 
         return the node. If the node was not a leaf, we would have to split the primitives into 
         left and right partitions of data, assigning the l and r to recursive calls to 
         construct_bvh, taking in data up to the splitting point and the data from the splitting 
         point to the end respectively. For choosing the splitting point, we first determined the
         average of all the centroids to act as the point that splits the data into two. However, 
         we had to then determine the best axis we would need to use to figure out which conditions 
         would be used for categorizing left and right data. To start, we calculated the minimum 
         and maximum x, y, and z values, and from this we determined the range of each axis by 
         subtracting each component’s maximum and minimum. Whichever axis has the largest range 
         would be the one to split as it would be most beneficial to split a longer range of data 
         rather than splitting thinner sets of data. After finding the axis, we used std::partition 
         to partition the primitives by selecting the left data as all those whose component of the 
         chosen axis was less than the average midpoint along our chosen axis. 
     </p>
     
      <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/lucy.png" align="middle" width="360px" />
            <figcaption align="middle">Statue</figcaption>
          </td>
          <td>
            <img src="images/man.png" align="middle" width="360px" />
            <figcaption align="middle">Man</figcaption>
          </td>
        </tr>
      </table>
    </div>
     
     <p>
         When rendering with the BVH acceleration the cow only takes 1.8284s, but without it 
         takes 78.0784s. Similarly the rendering for the teapot with BVH acceleration is only 
         2.473s, while without it is 151.0347s. These would render faster, but my computer is 
         not the best. The BVH acceleration speeds up the rendering by using recursion to 
         divide the work rather than going through all the primitives indualvidual. It splits 
         up the work. Therefore bring the runtime down from <i>O(n)O(n) to O(log(n))O(log(n))</i>.
     </p>
     
     <div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/cow2.png" align="middle" width="360px" />
            <figcaption align="middle">Cow</figcaption>
          </td>
          <td>
            <img src="images/teapot.png" align="middle" width="360px" />
            <figcaption align="middle">Teapot</figcaption>
          </td>
        </tr>
         </table>
     </div>

     
 <h2 align="middle">Part 3: Direct Illumination</h2>

<div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/CBbunny_H_64_32.png" align="middle" width="360px" />
            <figcaption align="middle">Uniform Hemisphere: Bunny</figcaption>
          </td>
          <td>
            <img src="images/bunny_64rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: Bunny</figcaption>
          </td>
        </tr>
        <br>
        <tr>
          <td>
            <img src="images/CBspheres_lambertian_H_64_32.png" align="middle" width="360px" />
            <figcaption align="middle">Uniform Hemisphere: Balls</figcaption>
            </td>
            <td>
            <img src="images/spheres_importance_sampling_64rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: Balls</figcaption>
          </td>
        </tr>
      </table>
    </div>

<p>
    <b>Uniform Hemisphere Sampling:</b> 
    First, we make a coordinate system for a hit point, and make sure that N is aligned with 
    the Z direction. Then we make w_out, which points towards the source of the ray, by 
    inverting the ray’s direction. We then for loop through all of the samples. For each 
    sample we normalize the sample and create another ray using the hit_p and world space. 
    We also offsetted the ray by EPS_F to avoid precision errors. Then we check if the ray 
    intersects the hit_p. If it does we update L-out to L_out + isect.bsdf->f(w_out, w_smpl) 
    * the emission of the intersection * dot(w_smpl_wrld, isect.n) / (1.0 / (2 * PI)).
    After the samples have been traversed through, we update L_out by averaging it, and then 
    return L_out. 
</p>

<p>
    <b>Importance Sampling: </b>
We implemented this similar to the one for uniform hemisphere. The differences are that we 
    for loop through all the scene lights. In that loop we check if the current light is the 
    delta light. If so, set num samples to 1. Then we transverse through all the samples. 
    If w2o * wi is negative then we move to the next sample. If not then we make a ray of
    hit_p and wi. Then we check if that ray intersects with the intersection. If it does we 
    update the current L_out to itself + isect.bsdf->f(w_out, wi_obj) * sample_radiance * 
    dot(wi, isect.n) / pdf. After all the samples have been looped through we average the 
    current L_out, and then update L_out to L_out + the averaged current L_out.
</p>

<div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/bunny_1rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: 1 Ray</figcaption>
          </td>
          <td>
            <img src="images/bunny_4 rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: 4 Rays</figcaption>
          </td>
        </tr>
        <br>
        <tr>
          <td>
            <img src="images/bunny_16 rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: 16 Rays</figcaption>
            </td>
            <td>
            <img src="images/bunny_64rays.png" align="middle" width="360px" />
            <figcaption align="middle">Importance: 64 Rays/figcaption>
          </td>
        </tr>
      </table>
    </div>

<p>
    In terms of more realistic features, uniform hemisphere sampling does appear to take on 
    more realistic forms of shading. Comparatively, importance sampling has a very sharp
    image, making the shading less realistic. The uniform hemisphere shading is more realistic,
    but the image is much noisier compared to the importance sampling one. We see here the
    tradeoff between the 2 different sampling techniques, realistic and noisy or shaper and 
    less noisy. 
</p>


      
 <h2 align="middle">Part 4: Global Illumination</h2>

<p>
    For the indirect lighting function, we had to implement two functions: at_least_one_bounce_radiance 
    and est_radiance_global_illumination. In at_least_one_bounce_radiance, our implementation 
    was similar to that of one_bounce_radiance in part 3 in that we used the given reflectance 
    function. However, this time, given certain conditions, we would recursively call the function
    to continually bounce to give more accurate rendering results. As mentioned in the spec, 
    these conditions are (1) the Russian roulette, in which we use a cpdf (or a continuous pdf 
    that is the complement of a termination probability) to calculate a random choice between 
    0 and 1 using the function coin_flip (to test if a random value between 0 and 1 is less 
    than the cpdf) to see if we will bounce once we begin reaching the maximum number of 
    bounces, and (2) max_ray_depth, or the maximum number of bounces allowed, is greater 
    than 1 in which the function will always continue with bouncing regardless of the Russian 
    roulette results. This, of course, would end regardless once the number of bounces 
    surpasses the maximum number of bounces defined by max_ray_depth. If the depth is 0, 
    we would just return Vector3D(0,0,0). We then add the result returned by this function 
    to the overall reflectance in est_radiance_global_illumination.
</p>

<div align="middle">
      <table style="width=100%">
        <tr>
          <td>
            <img src="images/spheres_direct.png" align="middle" width="360px" />
            <figcaption align="middle">Direct Illumination</figcaption>
          </td>
          <td>
            <img src="images/spheres_indirect.png" align="middle" width="360px" />
            <figcaption align="middle">Indirect Illumination</figcaption>
          </td>
        </tr>
      </table>
    </div>
           
 <h2 align="middle">Part 5: Adaptive Sampling</h2>
        
<p>
    For adaptive sampling, we limit the number of samples we take per pixel and instead focus more samples on pixels that need more samples for higher accuracy in order to reduce noise. To do so, we record several new variables including the current number of samples (which we increment until we decide to converge), s1 and s2 (which is the sum of illuminance values of the radiance and the same sum squared respectively), as well as the mean and standard deviation of the illuminance values and the convergence, which we calculate to be I = 1.96 * standard deviation / sqrt(curr_num_samples). To reduce the amount of computations, we also calculate convergences in batches of samplesPerBatch values, summing up s1 and s2 for each batch and after each batch computing the convergence. If the convergence I is less than or equal to the given maxTolerance * mean, we would stop generating rays and update the pixel with the color still calculated as the average radiance.

</p>
        
 
</div>
</body>
</html>
